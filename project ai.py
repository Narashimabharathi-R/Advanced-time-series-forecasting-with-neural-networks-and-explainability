# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e1mkQ-yLsaZoVmEfiFOjfw60X9oyXSfE
"""

# timeseries_lstm_project.py
"""
Advanced Time Series Forecasting (LSTM) with Explainability (SHAP)
Synthetic multivariate dataset -> LSTM multi-step forecasting -> baseline -> SHAP explainability
"""

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers
import matplotlib.pyplot as plt

# --- 1) Generate synthetic multivariate time series ---
def generate_synthetic_series(n_samples=2000, seed=42):
    np.random.seed(seed)
    t = np.arange(n_samples)

    # Feature 1: seasonal + trend + noise (target depends mainly on this)
    seasonal = 10 * np.sin(2 * np.pi * t / 24)  # daily seasonality
    trend = 0.01 * t
    noise = np.random.normal(scale=1.0, size=n_samples)
    target = 50 + seasonal + trend + noise

    # Feature 2: lagged version of target with some transformation
    feat_lagged = np.roll(target, 3) + np.random.normal(scale=0.5, size=n_samples)

    # Feature 3: exogenous binary holiday flag every ~100 days
    holiday = ((t % 100) < 3).astype(float)

    # Feature 4: temperature-like cyclic series
    temp = 20 + 5 * np.sin(2 * np.pi * t / (24*7)) + np.random.normal(scale=0.3, size=n_samples)

    # Feature 5: random noise / sensor drift
    drift = 0.001 * t + np.random.normal(scale=0.7, size=n_samples)

    df = pd.DataFrame({
        'target': target,
        'feat_lagged': feat_lagged,
        'holiday': holiday,
        'temp': temp,
        'drift': drift
    })
    # Fix first few rolled values
    df['feat_lagged'].iloc[:5] = df['feat_lagged'].iloc[5:10].values
    return df

# --- 2) Windowing & dataset creation ---
def create_windows(df, input_width=30, output_width=7, target_col='target'):
    """
    Returns:
     X: shape (num_windows, input_width, num_features)
     y: shape (num_windows, output_width)
     scaler: fitted scaler for inverse transform if needed
    """
    values = df.values.astype(np.float32)
    n_samples, n_feats = values.shape

    X = []
    y = []
    for end_idx in range(input_width, n_samples - output_width + 1):
        start_idx = end_idx - input_width
        X.append(values[start_idx:end_idx, :])
        y.append(values[end_idx:end_idx + output_width, 0])  # target is first column

    X = np.stack(X)  # (N, input_width, n_feats)
    y = np.stack(y)  # (N, output_width)
    return X, y

# --- 3) Train/val/test split and scaling ---
def split_and_scale(X, y, train_frac=0.7, val_frac=0.15):
    n = X.shape[0]
    train_end = int(n * train_frac)
    val_end = int(n * (train_frac + val_frac))

    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]

    # Flatten features to fit scaler per feature
    n_feats = X_train.shape[2]
    scaler = StandardScaler()
    X_train_flat = X_train.reshape(-1, n_feats)
    scaler.fit(X_train_flat)
    # Apply scaler on flattened then reshape back
    def scale_X(X):
        s = X.reshape(-1, n_feats)
        s = scaler.transform(s)
        return s.reshape(X.shape)
    X_train = scale_X(X_train)
    X_val = scale_X(X_val)
    X_test = scale_X(X_test)

    # Scale target separately (optional) - here we keep target in original scale for clarity,
    # because we will forecast raw target directly. If target scaling is desired, apply scaler.
    return X_train, y_train, X_val, y_val, X_test, y_test, scaler

# --- 4) LSTM model definition ---
def build_lstm_model(input_shape, output_steps, hidden_units=64, lr=1e-3):
    inputs = layers.Input(shape=input_shape)  # (input_width, n_feats)
    x = layers.LSTM(hidden_units, return_sequences=False)(inputs)
    x = layers.Dropout(0.2)(x)
    x = layers.Dense(hidden_units//2, activation='relu')(x)
    outputs = layers.Dense(output_steps)(x)  # regression outputs (no activation)
    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse')
    return model

# --- 5) Metrics helpers ---
def rmse(a, b):
    return math.sqrt(mean_squared_error(a, b))

def mape(y_true, y_pred):
    denom = np.maximum(np.abs(y_true), 1e-6)
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0

# --- 6) Baseline models ---
def persistence_baseline(X_test):
    # Predict next steps as the last observed target value in the input window
    # X_test shape (N, input_width, n_feats)
    last_targets = X_test[:, -1, 0]  # last target column
    # repeat across output horizon
    return np.tile(last_targets[:, None], (1, OUTPUT_WIDTH))

def moving_average_baseline(X_test, avg_window=3):
    # Use average of last `avg_window` target steps
    last_vals = []
    for i in range(X_test.shape[0]):
        last_vals.append(np.mean(X_test[i, -avg_window:, 0]))
    return np.tile(np.array(last_vals)[:, None], (1, OUTPUT_WIDTH))

# --- 7) Main execution ---
if __name__ == '__main__':
    # Hyperparameters
    N_SAMPLES = 2000
    INPUT_WIDTH = 30
    OUTPUT_WIDTH = 7
    BATCH_SIZE = 32
    EPOCHS = 30  # increase for better performance
    HIDDEN_UNITS = 128
    LR = 1e-3

    # 1. Generate synthetic dataset
    df = generate_synthetic_series(n_samples=N_SAMPLES)
    print("Dataset head:\n", df.head())

    # 2. Create windows
    X, y = create_windows(df, input_width=INPUT_WIDTH, output_width=OUTPUT_WIDTH)
    print("X shape:", X.shape, "y shape:", y.shape)

    # 3. Split and scale
    X_train, y_train, X_val, y_val, X_test, y_test, scaler = split_and_scale(X, y)
    print("Train/Val/Test shapes:", X_train.shape, X_val.shape, X_test.shape)

    # 4. Build & train LSTM
    INPUT_SHAPE = (INPUT_WIDTH, X_train.shape[2])
    OUTPUT_STEPS = OUTPUT_WIDTH
    model = build_lstm_model(INPUT_SHAPE, OUTPUT_STEPS, hidden_units=HIDDEN_UNITS, lr=LR)
    model.summary()

    chk = callbacks.ModelCheckpoint('best_lstm.h5', save_best_only=True, monitor='val_loss')
    es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=[chk, es],
        verbose=2
    )

    # 5. Evaluate on test set
    preds_test = model.predict(X_test)
    rmse_val = rmse(y_test.flatten(), preds_test.flatten())
    mae_val = mean_absolute_error(y_test.flatten(), preds_test.flatten())
    mape_val = mape(y_test, preds_test)
    print(f"LSTM Test RMSE: {rmse_val:.4f}, MAE: {mae_val:.4f}, MAPE: {mape_val:.2f}%")

    # 6. Baselines
    pers_preds = persistence_baseline(X_test)
    pers_rmse = rmse(y_test.flatten(), pers_preds.flatten())
    pers_mae = mean_absolute_error(y_test.flatten(), pers_preds.flatten())
    pers_mape = mape(y_test, pers_preds)
    print(f"Persistence RMSE: {pers_rmse:.4f}, MAE: {pers_mae:.4f}, MAPE: {pers_mape:.2f}%")

    ma_preds = moving_average_baseline(X_test, avg_window=3)
    ma_rmse = rmse(y_test.flatten(), ma_preds.flatten())
    ma_mae = mean_absolute_error(y_test.flatten(), ma_preds.flatten())
    ma_mape = mape(y_test, ma_preds)
    print(f"MovingAvg RMSE: {ma_rmse:.4f}, MAE: {ma_mae:.4f}, MAPE: {ma_mape:.2f}%")

    # 7. Small plot of a few test examples (uncomment to show)
    try:
        n_plot = 3
        plt.figure(figsize=(12, 6))
        for i in range(n_plot):
            idx = i
            plt.subplot(n_plot, 1, i+1)
            plt.plot(range(OUTPUT_WIDTH), y_test[idx], label='True', marker='o')
            plt.plot(range(OUTPUT_WIDTH), preds_test[idx], label='LSTM', marker='x')
            plt.plot(range(OUTPUT_WIDTH), pers_preds[idx], label='Persistence', linestyle='--')
            plt.legend()
            plt.title(f"Example {i}")
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print("Plot failed or running headless:", e)

    # 8. Explainability using SHAP (optional)
    # Note: SHAP DeepExplainer / GradientExplainer works with TensorFlow/Keras.
    # If shap isn't installed or causes issues, skip this block.
    try:
        import shap
        # We will explain predictions for the first 100 test windows using scaled inputs
        explainer = shap.GradientExplainer((model.input, model.output), X_train[:200])
        # shap_values shape: (num_samples, output_steps, input_shape?) - gradient explainer returns per-output attributions
        shap_values = explainer.shap_values(X_test[:100])
        # shap_values could be a list (one array per output step)
        # For convenience, aggregate absolute importance across output steps and time/features
        if isinstance(shap_values, list):
            # sum abs across outputs then average
            agg = np.sum([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)
        else:
            agg = np.abs(shap_values).mean(axis=0)

        print("SHAP explanation computed. Shape (time,features):", agg.shape)
        # Example: plot importance by feature averaged across input window
        feature_importance = agg.sum(axis=0)  # sum across time steps
        feature_names = df.columns.tolist()
        for fname, imp in zip(feature_names, feature_importance):
            print(f"{fname}: importance {imp:.4f}")

    except Exception as e:
        print("SHAP explainability skipped or failed:", e)

    # 9. Save model and sample outputs
    model.save('final_lstm_model')
    pd.DataFrame(preds_test).to_csv('predictions_test.csv', index=False)
    print("Model and predictions saved to disk.")